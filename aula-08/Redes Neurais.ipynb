{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fundamentos em Data Science\n",
    "\n",
    "## Redes Neurais Artificiais\n",
    "\n",
    "Fábio Sato <fabiosato@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Redes Neurais Artificiais (ANN) - Introdução\n",
    "\n",
    "Inspiração no cérebro humano para a construção de algoritmos inteligentes\n",
    "\n",
    "Algoritmos versáteis, poderosos e escaláveis que podem lidar com tarefas complexas de aprendizado de máquina\n",
    "\n",
    "Algoritmo base de Aprendizado Profundo (*Deep Learning*) que fornece os principais serviços de IA utilizados atualmente:\n",
    "    - Classificação de imagens (Google Image)\n",
    "    - Reconhecimento de fala (Siri)\n",
    "    - Recomendação de vídeos (Youtube e Netflix)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neurônios Biológicos\n",
    "\n",
    "Composto por um corpo celular contendo o núcleo, ramificações (dendritos) e uma longa extensão denominada axônio.\n",
    "\n",
    "O axônio apresenta ramificações em suas extremidades que possuem pequenas estruturas denominadas terminais sinápticos (sinapses) que estão conectadas aos dendritos de outros neurônios.\n",
    "\n",
    "Recebem curtos impulsos elétricos (sinais) de outros neurôrios através das sinapses.\n",
    "\n",
    "\n",
    "![Neurônio Biológico](figuras/biological-neuron.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neurônios Biológicos\n",
    "\n",
    "Quando um neurônio recebe um número suficiente de sinais de outros neurônios dentro \n",
    "de alguns milisegundos ele dispara seu próprio sinal.\n",
    "\n",
    "Neurônios individuais se comportam de forma bastante simples, mas estão organizados em uma vasta rede de bilhões de neurônios, onde cada neurônio está conectado a milhares de outros neurônios.\n",
    "\n",
    "\"Cálculos\" altamente complexos podem ser realizados por uma vasta rede de simples neurônios.\n",
    "\n",
    "![Biological Neurons Layers](figuras/biological-neurons-layers.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ANN - Histórico\n",
    "\n",
    "## 1943 - McCulloch e Pitts\n",
    "\n",
    "*A Logical Calculus of Ideas Immanent in Nervous Activity*\n",
    "\n",
    "Primeira arquitetura de redes neurais artificiais.\n",
    "\n",
    "Modelo computacional simplificado de como neurônios biológicos podem trabalhar em conjunto para realizar operações complexas.\n",
    "\n",
    "Uma ou mais entradas binárias (ligado/desligado) e uma saída binária.\n",
    "\n",
    "Ativação da saída quando mais do que um certo número de entradas estão ativas.\n",
    "\n",
    "![McCulloch and Pitts ANN](figuras/mcculloch-pitts-ann.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ANN - Histórico\n",
    "\n",
    "## 1957: Frank Rosenblat\n",
    "\n",
    "Proposta de um neurônio artifical ligeiramente modificado.\n",
    "\n",
    "LTU - *Linear Treshold Unit* - entradas e saídas são números e um peso é associado a cada conexão de entrada.\n",
    "\n",
    "Cálculo da soma ponderada das entradas: $ z = w_1 x_1 + w_2 x_2 + ... + w_n x_n = \\mathbf{w}^T \\cdot \\mathbf{x}$\n",
    "\n",
    "Função de *step* sobre o soma que produz na saída o resultado $h_w(\\mathbf{x}) = step(z)$\n",
    "\n",
    "![Perceptron](figuras/perceptron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LTU - Funções Step\n",
    "\n",
    "\n",
    "$$ heaviside(z) = \n",
    "\\begin{cases}\n",
    "0 & if z \\lt 0 \\\\\n",
    "1 & if z \\ge 0\n",
    "\\end{cases} $$\n",
    "\n",
    "$$ sign(z) = \n",
    "\\begin{cases}\n",
    "-1 & if z \\lt 0 \\\\\n",
    "0  & if z = 0 \\\\\n",
    "+1 & if z \\gt 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Perceptron\n",
    "\n",
    "Combinação linear das entradas: LTU pode ser usado somente para problemas simples de classificação binária linear.\n",
    "\n",
    "Um Perceptron consiste de uma camada simples de LTUs onde cada neurônio está conectado a todas entradas.\n",
    "\n",
    "As conexões de entrada são compostas por neurônios que reproduzem os valores na saída.\n",
    "\n",
    "Um neurônio de viés (*bias*) é geralmente adicionado ($x_0 = 1$).\n",
    "\n",
    "Perceptrons não são capazes de aprender padrões complexos.\n",
    "\n",
    "Entretanto, Rosenblat demonstrou que se o problema é linearmente separável o algoritmo irá convergir para uma solução.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Perceptron - Treinamento\n",
    "\n",
    "Baseado na regra de Hebb: quando um neurônio frequentemente ativa outro a conexão entre eles fica mais forte.\n",
    "\n",
    "Perceptrons são treinados de forma que o erro produzido pela rede seja levado em consideração.\n",
    "\n",
    "Não reforça conexões que conduziram a uma saída errada.\n",
    "\n",
    "Para cada instância de treinamento o erro da rede é determinado e os pesos são atualizados de forma que as conexões de entradas que poderiam ter contribuido para uma saída correta sejam reforçadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Perceptron - Regra de Aprendizado\n",
    "\n",
    "$$ w_{i,j}^{(next step)} = w_{i, j} + \\eta (\\hat{y_i} - y_i)x_i $$\n",
    "\n",
    "- $w_{i,j}$ é o peso da conexão entre o $i$-ésimo neurônio de entrada e o $j$-ésimo neurônio de saída\n",
    "- $x_i$ é o $i$-ésimo valor de entrada da instância de treinamento atual\n",
    "- $\\hat{y_i}$ é a saída do $j$-ésimo neurônio de saída para a instância de treinamento atual\n",
    "- $y_i$ é o valor de saída alvo do $j$-ésimo neurônio de saída para a instância de treinamento atual\n",
    "- $\\eta$ é a taxa de aprendizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Perceptron - Exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sato/treinamentos/fundamentos-data-science/env/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2,3)]\n",
    "y = (iris.target == 0).astype(np.int)\n",
    "\n",
    "clf = Perceptron(random_state=1234)\n",
    "clf.fit(X, y)\n",
    "\n",
    "y_pred = clf.predict(X)\n",
    "print(\"Acurácia: %f\" % accuracy_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MLP - Multi-Layer Perceptron\n",
    "\n",
    "1969 - Minksy e Papert: limitações do Perceptron (XOR)\n",
    "\n",
    "MLP: algumas das limitações podem ser eliminadas através do empilhamento de múltiplos Perceptrons.\n",
    "\n",
    "Uma MLP é composta por uma ou mais camadas de entrada, um ou mais camadas de LTUs (camadas escondidas), e uma camada final de LTUs chamada de camada de saída.\n",
    "\n",
    "Quando uma ANN possui duas ou mais camadas escondidas é denominada rede neural profunda (DNN - *Deep Neural Network*)\n",
    "\n",
    "![MLP](figuras/mlp-network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP - Treinamento\n",
    "\n",
    "Durante anos muitos pesquisadores tentaram encontrar uma forma de treinar MLPs sem sucesso.\n",
    "\n",
    "Em 1986 Rumelhart et al. publicaram um artigo introduzindo o algoritmo de treinamento *Backpropagation*.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
