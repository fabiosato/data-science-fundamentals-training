{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fundamentos em Data Science\n",
    "\n",
    "## Aula 5 - Algoritmos de Ensemble\n",
    "\n",
    "- Bagging\n",
    "- Boosting\n",
    "- Stacking\n",
    "- Random Forests\n",
    "\n",
    "Fábio Sato - <fabiosato@gmail.com>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ensemble\n",
    "\n",
    "Meta algoritmos que combinam várias técnicas de aprendizado de máquina em um único modelo preditivo\n",
    "\n",
    "Possibilita obter um desempenho preditivo melhor do que o obtido por um modelo único\n",
    "\n",
    "Objetivo: melhorar a predição de um modelo base (classificador ou regressor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ensemble\n",
    "\n",
    "<img src=\"figuras/ensemble-learning.png\" alt=\"Ensemble Learning\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ensemble - Tipos de Métodos\n",
    "\n",
    "Os métodos de ensemble podem ser divididos em dois grupos principais:\n",
    "\n",
    "- Sequencial\n",
    "- Paralelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ensemble - Métodos Sequenciais\n",
    "\n",
    "Modelos base são gerados sequencialmente\n",
    "\n",
    "Explora a **DEPENDÊNCIA** entre os modelos base\n",
    "\n",
    "O desempenho global pode ser melhorado atribuindo pesos maiores a exemplos classificados erroneamente pelo modelo anterior\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ensemble - Métodos Paralelos\n",
    "\n",
    "Modelos base são gerados em paralelo\n",
    "\n",
    "Explorar a **INDEPENDÊNCIA** entre os modelos base\n",
    "\n",
    "Erros podem ser reduzidos drasticamente através da média"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ensemble - Homogêneo x Heterogêneo\n",
    "\n",
    "**Homogêneo**: A maioria dos métodos de ensemble utilizam um algoritmo base de aprendizado\n",
    "para produzir modelos do mesmo tipo\n",
    "\n",
    "**Heterogêneo**: Existem alguns métodos que utilizam algoritmos heterogêneos (de diferentes tipos)\n",
    "\n",
    "Para que os métodos baseados em ensemble apresentem maior acurácia do que qualquer\n",
    "um de seus membros individuais, os modelos de base devem ser o mais precisos e **DIVERSOS** possíveis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ensemble - Bagging\n",
    "\n",
    "Bagging: *Boostrap Aggregation*\n",
    "\n",
    "Redução da variância nas estimativas através da agregação (média) de múltiplas estimativas\n",
    "\n",
    "Treina algoritmos em diferentes subconjuntos de dados\n",
    "\n",
    "As bases de treinamento para cada modelo base são geradas através de amostragem aleatória com reposição \n",
    "\n",
    "Para agregar as saídas dos modelos base, bagging utiliza o **voto** para classificação e **média** para regressão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ensemble - Bagging\n",
    "\n",
    "$$ f(x) = \\frac{1}{M}\\sum_{m=1}^{M}{f_{m}(x)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ensemble - Bagging: Exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): mlxtend in /Users/Sato/dev/treinamentos/fundamentos-data-science/env/lib/python3.6/site-packages\r\n",
      "Requirement already satisfied (use --upgrade to upgrade): setuptools in /Users/Sato/dev/treinamentos/fundamentos-data-science/env/lib/python3.6/site-packages (from mlxtend)\r\n",
      "\u001b[33mYou are using pip version 7.1.2, however version 9.0.1 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "from mlxtend.plotting import plot_learning_curves\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, 0:2], iris.target\n",
    "    \n",
    "clf1 = DecisionTreeClassifier(criterion='entropy', max_depth=1)\n",
    "clf2 = KNeighborsClassifier(n_neighbors=1)    \n",
    "\n",
    "bagging1 = BaggingClassifier(base_estimator=clf1, n_estimators=10, \n",
    "                             max_samples=0.8, max_features=0.8)\n",
    "\n",
    "bagging2 = BaggingClassifier(base_estimator=clf2, n_estimators=10, \n",
    "                             max_samples=0.8, max_features=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "label = ['Decision Tree', 'K-NN', 'Bagging Tree', 'Bagging K-NN']\n",
    "clf_list = [clf1, clf2, bagging1, bagging2]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "grid = itertools.product([0,1],repeat=2)\n",
    "\n",
    "for clf, label, grd in zip(clf_list, label, grid):        \n",
    "    scores = cross_val_score(clf, X, y, cv=3, scoring='accuracy')\n",
    "    print(\"Accuracy: %.2f (+/- %.2f) [%s]\" %(scores.mean(), scores.std(), label))\n",
    "        \n",
    "    clf.fit(X, y)\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n",
    "    plt.title(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Random Forests\n",
    "\n",
    "Um exemplo bem comum de classe de algoritmo de ensemble são as florestas aleatórias de árvores (Random Forests)\n",
    "\n",
    "Em Random Forests, cada árvore é um ensemble construído a partir de uma amostra com substituição.\n",
    "\n",
    "Além disso, ao invés de utilizar todas as características, um conjunto aleatório de características é selecionado\n",
    "tornando o processo de criação de árvores ainda mais aleatório.\n",
    "\n",
    "Como resultado, o viés da floresta cresce ligeiramente mas a variância diminui significativamente resultando em um modelo muito melhor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Random Forests\n",
    "\n",
    "![Random Forests](figuras/random-forests.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Random Forests - Exercício\n",
    "\n",
    "Implemente um modelo baseado em Random Forests para a base de dados 'Boston House Prices'\n",
    "\n",
    "<https://archive.ics.uci.edu/ml/machine-learning-databases/housing/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ensemble - Boosting\n",
    "\n",
    "Família de algoritmos que convertem classificadores fracos em algoritmos de aprendizado fortes.\n",
    "\n",
    "Princípio fundamental: treinar uma sequência de classificadores fracos que são apenas um pouco melhores que adivinhar aleatoriamente (ex: pequenas árvores de decisão)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ensemble - Boosting\n",
    "\n",
    "Os classificadores fracos são treinados em versões com peso ponderado dos dados. Mais peso é atribuído aos exemplos que foram classificadores errôneamente pelo modelo anterior.\n",
    "\n",
    "As previsões são combinadas usando voto ponderado (classificação) ou soma ponderada (regressão) para produzir a estimativa final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Boosting - AdaBoost\n",
    "\n",
    "Os classificadores fracos no AdaBoost são árvores de decisão com profundida igual a 1, chamados de *stumps* (tocos).\n",
    "\n",
    "O AdaBoost pondera as observações, colocando mais peso na dificuldade de classificar exemplos e menos naqueles que já\n",
    "são classificados corretamente.\n",
    "\n",
    "Novos classificadores fracos são adicionados sequencialmente e focam seu treinamento nos padrões mais difíceis.\n",
    "\n",
    "Exemplos que são difíceis de serem classificados recebem cada vez mais peso até que o algoritmo os classifiquem\n",
    "corretamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Boosting: AdaBoost\n",
    "\n",
    "![AdaBoost](figuras/adaboost.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Boosting: Exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "from mlxtend.plotting import plot_learning_curves\n",
    "from mlxtend.plotting import plot_decision_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, 0:2], iris.target\n",
    "    \n",
    "clf = DecisionTreeClassifier(criterion='entropy', max_depth=1)\n",
    "\n",
    "num_est = [1, 2, 3, 10]\n",
    "label = ['AdaBoost (n=1)', 'AdaBoost (n=2)', 'AdaBoost (n=3)', 'AdaBoost (n=10)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "grid = itertools.product([0,1],repeat=2)\n",
    "\n",
    "for n_est, label, grd in zip(num_est, label, grid):     \n",
    "    boosting = AdaBoostClassifier(base_estimator=clf, n_estimators=n_est)   \n",
    "    boosting.fit(X, y)\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig = plot_decision_regions(X=X, y=y, clf=boosting, legend=2)\n",
    "    plt.title(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bagging vs Boosting: Bases\n",
    "\n",
    "![Bagging vs Boosting](figuras/bagging-boosting-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bagging vs Boosting: Sequencial vs Paralelo\n",
    "    \n",
    "![Bagging vs Boosting](figuras/bagging-boosting-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bagging vs Boosting: Treinamento\n",
    "\n",
    "![Bagging vs Boosting](figuras/bagging-boosting-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bagging vs Boosting: qual é o melhor?\n",
    "\n",
    "Não há um vencedor: depende dos dados, da complexidade do problema e do algoritmo base\n",
    "\n",
    "Bagging e Boosting reduzem a variância de uma estimativa única. Modelo com maior estabilidade.\n",
    "\n",
    "Bagging raramente irá melhorar o viés de um modelo único com baixo desempenho. Entretanto pode gerar um modelo combinado com menores erros.\n",
    "\n",
    "Se o problema do modelo único é o overfitting, então Bagging é a melhor opção. Boosting não ajuda a evitar/melhorar o overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stacking\n",
    "\n",
    "Stacking é uma técnica de aprendizado por ensemble que combina múltiplos modelos de classificação\n",
    "ou regressão através de um meta classificador ou meta regressor.\n",
    "\n",
    "Os modelos de base são treinados em um conjunto completo da base de treinamento, e então o meta modelo utiliza as\n",
    "saídas dos modelos de base como características de entrada.\n",
    "\n",
    "A base geralmente utiliza diferentes algoritmos de aprendizado e portanto geralmente stacking se refere a\n",
    "um modelo heterogêneos de ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stacking\n",
    "\n",
    "![Stacking](figuras/stacking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stacking: Exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "from mlxtend.plotting import plot_learning_curves\n",
    "from mlxtend.plotting import plot_decision_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, 1:3], iris.target\n",
    "\n",
    "clf1 = KNeighborsClassifier(n_neighbors=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "lr = LogisticRegression()\n",
    "sclf = StackingClassifier(classifiers=[clf1, clf2, clf3], \n",
    "                          meta_classifier=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "label = ['KNN', 'Random Forest', 'Naive Bayes', 'Stacking Classifier']\n",
    "clf_list = [clf1, clf2, clf3, sclf]\n",
    "    \n",
    "fig = plt.figure(figsize=(10,8))\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "grid = itertools.product([0,1],repeat=2)\n",
    "\n",
    "clf_cv_mean = []\n",
    "clf_cv_std = []\n",
    "for clf, label, grd in zip(clf_list, label, grid):\n",
    "        \n",
    "    scores = cross_val_score(clf, X, y, cv=3, scoring='accuracy')\n",
    "    print(\"Accuracy: %.2f (+/- %.2f) [%s]\" %(scores.mean(), scores.std(), label))\n",
    "    clf_cv_mean.append(scores.mean())\n",
    "    clf_cv_std.append(scores.std())\n",
    "        \n",
    "    clf.fit(X, y)\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig = plot_decision_regions(X=X, y=y, clf=clf)\n",
    "    plt.title(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "   plt.figure()\n",
    "(_, caps, _) = plt.errorbar(range(4), clf_cv_mean, yerr=clf_cv_std, c='blue', fmt='-o', capsize=5)\n",
    "for cap in caps:\n",
    "    cap.set_markeredgewidth(1)                                                                                                                                \n",
    "plt.xticks(range(4), ['KNN', 'RF', 'NB', 'Stacking'])        \n",
    "plt.ylabel('Accuracy'); plt.xlabel('Classifier'); plt.title('Stacking Ensemble');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "plt.figure()\n",
    "plot_learning_curves(X_train, y_train, X_test, y_test, sclf, print_model=False, style='ggplot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
