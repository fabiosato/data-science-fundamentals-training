{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fundamentos em Data Science\n",
    "\n",
    "## Support Vector Machine\n",
    "\n",
    "Fábio Sato <fabiosato@gmail.com>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Instalação de pacotes adicionais\n",
    "\n",
    "Antes de iniciarmos execute os seguintes comandos:\n",
    "\n",
    "```\n",
    "pip install ipywidgets\n",
    "jupyter nbextension enable --py widgetsnbextension\n",
    "```\n",
    "\n",
    "Reinicialize o Jupyter se ele já estiver rodando"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SVM - Support Vector Machine\n",
    "\n",
    "\n",
    "Uma máquina de vetores de suporte (SVM) é um algoritmo de aprendizado de máquina bastante flexível capaz de resolver problemas lineares e não-lineares de classificação e de regressão.\n",
    "\n",
    "É atualmente um dos algoritmos mais populares e são adequados para modelos complexos com conjuntos de dados pequenos e médios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SVM - Classificação Linear \n",
    "\n",
    "Considere as seguintes considerações sobre um exemplo onde dados de duas classes podem ser separados linearmente.\n",
    "\n",
    "![SVM linear](figuras/svm1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SVM - Classificação Linear \n",
    "\n",
    "- A linha vermelha separa os dados mas pode não ter bom desempenho/acurácia para novos exemplos.\n",
    "\n",
    "- Diversas linhas podem classificar os dados corretamente.\n",
    "\n",
    "- A linha azul separa os dados e também mantém uma boa distância dos exemplos de treinamento de ambas as classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SVM - Large Margin Classification\n",
    "\n",
    "Um modelo SVM deve produzir uma fronteira linear (hiperplanos) que classifica dados de maneira ótima.\n",
    "\n",
    "Pode-se imaginar o trabalho de um classificador SVM como o de ajustar a rua mais larga possível (linhas tracejadas) entre duas classes.\n",
    "\n",
    "![center SVM 2](figuras/svm2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SVM - Support Vectors\n",
    "\n",
    "A melhor fronteira de decisão possível é determinada (\"suportada\") por exemplos localizadas na borda da rua.\n",
    "\n",
    "Estes exemplos são denominados vetores de suporte. A distância entre as bordas da rua é chamada de margem.\n",
    "\n",
    "![150% center SVM Support Vectors](figuras/svm-support-vectors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SVM - Hard Margin\n",
    "\n",
    "Hard Margin: exemplos devem se manter fora da \"rua\" e do lado correto da linha\n",
    "\n",
    "Dois problemas com esta abordagem\n",
    "\n",
    "1. Funciona somente para dados que são linearmente separáveis\n",
    "2. O algoritmo se torna muito sensível a outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SVM - Outliers\n",
    "\n",
    "Considere agora este exemplo onde temos um outlier da classe azul\n",
    "\n",
    "<img src=\"figuras/svm-outliers.png\" style=\"height: 350px;text-align:center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SVM - Soft Margin\n",
    "\n",
    "Se adotarmos um algoritmo de *hard margin* a fronteira de decisão ficará muito estreita.\n",
    "\n",
    "É melhor utilizar um modelo mais flexível.\n",
    "\n",
    "Soft Marging: determina um bom custo/benefício entre manter a rua o mais larga possível e limitar a violação da margem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SVM - Classificação Não-Linear\n",
    "\n",
    "Mundo real: embora o algoritmo SVM linear seja eficiente e flexível em muitos cenários nossos problemas não são lineares\n",
    "\n",
    "<img src=\"figuras/classification-nonlinear.png\" alt=\"Classification Non-Linear\" style=\"height: 350px;text-align: center;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SVM - Kernel Trick\n",
    "\n",
    "Kernel Trick: podemos resolver problemas não lineares projetando as características em um espaço de maiores dimensões onde o problema é linearmente separável\n",
    "\n",
    "Kernel é uma forma de calcular o produto interno de dois vetores $\\mathbf{x}$ e $\\mathbf{y}$ em um espaço de características de altas dimensões."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SVM - Funções de Kernel\n",
    "\n",
    "Suponha que uma função de mapeamento $\\varphi : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ que projetem vectores no espaço\n",
    "$\\mathbb{R}^n$ para o espaço de características $\\mathbb{R}^m$.\n",
    "\n",
    "O produto interno de $\\mathbf{x}$ e $\\mathbf{y}$ neste espaço é $\\mathbf{\\varphi(x)^T \\varphi(y)}$.\n",
    "\n",
    "Um kernel é uma função $k$ que corresponde a este produto interno: $k(\\mathbf{x, y}) = \\mathbf{\\varphi(x)^T \\varphi(y)}$\n",
    "\n",
    "Kernels fornecem uma maneira de calcular produtos internos em algum espaço de características sem necessariamente sabermos qual o espaço e $\\mathbf{\\varphi}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SVM - Kernel Polinomial\n",
    "\n",
    "A adição manual de características polinomiais é de simples implementação.\n",
    "\n",
    "Entretanto, polinômios com grau baixo não irão conseguir lidar com problemas complexos.\n",
    "\n",
    "Polinômios com alto grau irão gerar um número gigantesco de novas características.\n",
    "\n",
    "Para resolver isto, podemos usar um kernel polinomial: \n",
    "\n",
    "$$ k(x,y) = (\\mathbf{x}^T \\mathbf{y} + 1)^{d}$$\n",
    "\n",
    "Onde $d$ é o grau do polinômio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SVM - Kernel de Base Radial (RBF)\n",
    "\n",
    "O Gaussian RBF (Radial Basis Function) é um outro método de kernel popular utilizado em SVM.\n",
    "\n",
    "Kernel gaussianos possuem o seguinte formato $$ k(\\mathbf{x},\\mathbf{y}) = e^{-\\gamma\\lvert x - y\\rvert^2}, \\gamma \\gt 0 $$\n",
    "\n",
    "<img src=\"figuras/svm-rbf.png\" style=\"height: 350px;text-align: center;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SVM - Hiperparâmetros\n",
    "\n",
    "Existem dois hiperparâmetros importantes num modelo SVM:\n",
    "\n",
    "- C: define a largura da margem do classificador.\n",
    "- $\\gamma$: define o \"raio\" de influência de cada exemplo de treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SVM - Parâmetro C\n",
    "\n",
    "Valores grandes de C tornam o classificador rígido e portanto com uma menor margem.\n",
    "\n",
    "Para valores grandes de C o modelo irá escolher hiperplanos com menores margens se o hiperplano conseguir classificar os exemplos de treinamento corretamente\n",
    "\n",
    "Valores pequenos de C irão fazer o algoritmo procurar por hiperplanos que apresentem margens de separação mais largas, mesmo que o hiperplano classifique incorretamente mais pontos.\n",
    "\n",
    "Valores muito pequenos de C irão produzir classificações erradas com mais frequencia, mesmo que problema seja linearmente separável\n",
    "\n",
    "<img src=\"figuras/svm-c.png\" style=\"height: 200px;text-align: center;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SVM - Parâmetro $\\gamma$\n",
    "\n",
    "Define e alcance da influência de cada exemplo de treinamento.\n",
    "\n",
    "No Scikit-Learn este parâmetro é inválido para o kernel linear.\n",
    "\n",
    "\n",
    "<img src=\"figuras/svm-gamma.png\" style=\"height: 350px;text-align: center;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SVM - Exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d8bc0e6924421a97c7b567fde6af33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(Dropdown(description='C', options=(1, 10, 100, 1000, 10000, 100000), value=1), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact, widgets\n",
    "\n",
    "@interact(C=[1,10,100,1000,10000, 100000])\n",
    "def train_svm(C):\n",
    "    svc = svm.SVC(kernel='linear', C=C,gamma=0.1)\n",
    "    svc.fit(X, y)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    h = (x_max / x_min)/100\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "     np.arange(y_min, y_max, h))\n",
    "\n",
    "    plt.subplot(1, 1, 1)\n",
    "    Z = svc.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n",
    "\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n",
    "    plt.xlabel('Sepal length')\n",
    "    plt.ylabel('Sepal width')\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.title('SVC with linear kernel')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SVM - Exercício: implemente SVMs com kernels polinomiais e RBF para o mesmo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
